import streamlit as st

from config import MODEL_PATH, DATA_PATH
from retriever import Retriever
from reader.predictor import Predictor, QuestionContextInput


@st.cache_resource
def load_resources():
    r = Retriever(DATA_PATH)
    p = Predictor(MODEL_PATH)
    return r, p


with st.spinner('Äang táº£i mÃ´ hÃ¬nh...'):
    retriever, predictor = load_resources()


with st.sidebar:
    st.title('ğŸ’¬ Simple Question Answering System')

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "Há»i tÃ´i Ä‘i"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "Há»i tÃ´i Ä‘i"}]


st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

user_msg = st.chat_input('Nháº­p cÃ¢u há»i')
if user_msg:
    st.session_state.messages.append({"role": "user", "content": user_msg})
    with st.chat_message('user'):
        st.write(user_msg)

    documents = retriever.find_relevant_documents(user_msg, k=5)
    st.sidebar.write('CÃ¡c tÃ i liá»‡u liÃªn quan:')
    st.sidebar.write(documents)
    qci = QuestionContextInput(question=user_msg, context=documents[0]['context'])
    answer = predictor.answer([qci])
    print(answer)

    st.session_state.messages.append({"role": "assistant", "content": answer['answer']})
    with st.chat_message('assistant'):
        st.write(answer['answer'])
